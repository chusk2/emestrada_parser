{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selectividad Exams Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required packages for OCR processing:\n",
    "**Ubuntu based:**\n",
    "- tesseract-ocr\n",
    "- tesseract-ocr-spa\n",
    "- poppler-utils\n",
    "\n",
    "**Archlinux based:**\n",
    "- tesseract\n",
    "- tesseract-data-spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdf2image\n",
    "# !pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telegram bot\n",
    "\n",
    "Function to generate messages to a telegram bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your bot's token and chat ID\n",
    "from bot_credentials import *\n",
    "\n",
    "def send_telegram_message(message):\n",
    "    url = f\"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage\"\n",
    "    params = {\"chat_id\": CHAT_ID, \"text\": message}\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *pdf_to_text*\n",
    "\n",
    "Extract the text from pdf files, page by page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(file_path):\n",
    "    \"\"\"\n",
    "    Processes a single PDF file to extract exam information and statements.\n",
    "\n",
    "    Args:\n",
    "        file (Path): A Path object representing the PDF file to be processed.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted information with the following structure:\n",
    "            {\n",
    "                \"key\": {\n",
    "                    \"subject\": list of subjects,\n",
    "                    \"year\": list of years,\n",
    "                    \"topic\": list of topics,\n",
    "                    \"exam\": list of exams,\n",
    "                    \"exercise\": list of exercises,\n",
    "                    \"statement\": list of statements\n",
    "                }\n",
    "            }\n",
    "\n",
    "    Workflow:\n",
    "        1. Extracts the topic from the file name.\n",
    "        2. Converts the PDF file into text, page by page.\n",
    "        3. For each page:\n",
    "            - Extracts the statement using the `extract_statement` function.\n",
    "            - Extracts exam details using the `extract_exam_details` function.\n",
    "            - Handles errors by logging them using `generate_error_log`.\n",
    "        4. Aggregates the extracted information into lists for subjects, years, topics, exams, exercises, and statements.\n",
    "        5. Returns a dictionary with the extracted data.\n",
    "\n",
    "    Notes:\n",
    "        - Logs errors to a file if there are issues with extracting statements or exam details.\n",
    "        - Skips pages with errors and continues processing the remaining pages.\n",
    "    \"\"\"\n",
    "    images = convert_from_path(file_path)[1:]\n",
    "    # parse each page\n",
    "    pages = []\n",
    "    for i, page in enumerate(images):\n",
    "        pages.append(pytesseract.image_to_string(page, lang='spa'))\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *extract_statement*\n",
    "Extract the statement from a page in text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statement(page):\n",
    "    # split the content into lines\n",
    "    text = page.split('\\n')\n",
    "\n",
    "    # list to store the processed lines\n",
    "    final_text = []\n",
    "    # start from the 4th line\n",
    "    for line in text[4:]:\n",
    "\n",
    "        # check the first word of the line\n",
    "        first_word = line.replace('.', '').split(' ')[0].strip().upper()\n",
    "\n",
    "        # stop at examen details line\n",
    "        if first_word in ['SOCIALES', 'FISICA', 'MATES',\n",
    "                          'QUÍMICA', 'QUIMICA', 'QUIÍMICA']:\n",
    "            final_text.append(line.strip())\n",
    "            break\n",
    "        \n",
    "        # add the line to final_text\n",
    "        else:\n",
    "            if not line == '':\n",
    "                final_text.append(line.strip())\n",
    "\n",
    "    # join the list into a single string\n",
    "    final_text = '\\n'.join(final_text)\n",
    "\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'a' in ['a', 'b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *extract_exam_details*\n",
    "Extract the exam details, like subject, year, exam and exercise number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_exam_details(statement):\n",
    "    # get last line with exam details\n",
    "    lines = [i for i in statement.split('\\n') if i != '']\n",
    "    exam_details = lines[-1].lower()\n",
    "\n",
    "    # strip whitespaces\n",
    "    exam_details = (exam_details.replace('.', '')            \n",
    "                    .replace(',', '')\n",
    "                    .replace('(', '')\n",
    "                    .replace(')', '').strip()\n",
    "    )\n",
    "    \n",
    "    # split the words by ' '\n",
    "    exam_details = exam_details.split(' ')\n",
    "\n",
    "    # MATES CCSS\n",
    "    # SOCIALES II. 2017 JUNIO. EJERCICIO 2. OPCIÓN A\n",
    "    # \n",
    "    if exam_details[0].startswith('sociales'):\n",
    "        exam_details[0] = 'MATES_CCSS'\n",
    "        # remove the second element in the list\n",
    "        del exam_details[1]\n",
    "    \n",
    "       \n",
    "    # exam dictionary\n",
    "    details = ['subject', 'year', 'exam', 'exercise']\n",
    "    exam_dict = dict.fromkeys(details)\n",
    "    exam_dict = {k:None for k in details}\n",
    "\n",
    "    ## fill dictionary with values\n",
    "    \n",
    "    # subject\n",
    "    exam_dict['subject'] = exam_details[0]\n",
    "    \n",
    "    # year\n",
    "    # SOCIALES II. PONENCIA 2009. EJERCICIO 2\n",
    "    if exam_details[1] == 'ponencia':\n",
    "        # switch ponencia and year\n",
    "        exam_details[1], exam_details[2] = exam_details[2], exam_details[1]\n",
    "\n",
    "    exam_dict['year'] = int(exam_details[1])\n",
    "\n",
    "    # parse the exam string\n",
    "    if exam_details[2].startswith('reserva'):\n",
    "        exam_dict['exam'] = ' '.join(exam_details[2:4]).title()\n",
    "        exam_index_start = 4\n",
    "    else:\n",
    "        exam_dict['exam'] = exam_details[2].title()\n",
    "        exam_index_start = 3\n",
    "    \n",
    "    exam_dict['exercise'] = ' '.join(exam_details[exam_index_start:]).title()\n",
    "\n",
    "    \n",
    "    return exam_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *time*\n",
    "Function to measure the time taken to process the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def what_time():\n",
    "    now = time.localtime()\n",
    "    time_formated = time.strftime(\"%H:%M:%S on %Y-%m-%d \", now)\n",
    "    return now, time_formated\n",
    "\n",
    "def elapsed_time_minutes(start, end):\n",
    "    start_seconds = time.mktime(start)\n",
    "    end_seconds = time.mktime(end)\n",
    "    return (end_seconds - start_seconds) / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *generate_error_log*\n",
    "generate a error log and add it to errors_timestamp.log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_error_log(file, page, error_type, statement = None, details = None):\n",
    "    log = f'Error processing {error_type}, in {file.name} at page {page+2}\\n'\n",
    "    \n",
    "    # add timestamp to log file\n",
    "    current_datetime = datetime.now()\n",
    "    formatted_dt = current_datetime.strftime(\"%d-%m-%Y\")\n",
    "    with open(f'./error_logs/errors_{formatted_dt}.log', 'a') as f:\n",
    "        f.write(log)\n",
    "        f.write('*' * 10 + '\\n')\n",
    "        if statement:\n",
    "            f.write(f'{statement}\\n')\n",
    "        elif details:\n",
    "            f.write(f'{details}\\n')\n",
    "        f.write('*' * 10 + '\\n')\n",
    "    \n",
    "    # print log to console without the new line\n",
    "    print(log[:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *add_row*\n",
    "Manually add a row to the processed exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_row(df, statement, exam_details):\n",
    "    \n",
    "    row = extract_exam_details(exam_details)\n",
    "    row['statement'] = statement\n",
    "\n",
    "    return pd.concat([df, pd.DataFrame(row, columns = df.columns, index = [0])], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *process_file*\n",
    "Process a pdf files, extracting its information: statement and exam information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file: str) -> dict:\n",
    "    exercises_dict = {}\n",
    "    file = Path(file)\n",
    "    topic = file.stem.split(' - ')[-1]   \n",
    "    \n",
    "    # lists to store the values\n",
    "    subjects = []\n",
    "    years = []\n",
    "    exams = []\n",
    "    exercises = []\n",
    "    statements = []\n",
    "    pages = pdf_to_text(file)\n",
    "\n",
    "    # parse each page in the pdf file\n",
    "\n",
    "    output = f'Success parsing file: {file.stem}'\n",
    "    \n",
    "    for index, page in enumerate(pages):\n",
    "\n",
    "        # extract statement from page\n",
    "        try:\n",
    "            statement = extract_statement(page)\n",
    "        # error with extracting statement\n",
    "        except:\n",
    "            for line in statement.split('\\n'):\n",
    "                if line.startswith('www.emestrada.org'):\n",
    "                    continue\n",
    "            generate_error_log(file = file, page = index, error_type= 'statement', statement=page)\n",
    "            continue\n",
    "\n",
    "        # extract exam details\n",
    "        try:\n",
    "            details = extract_exam_details(statement)\n",
    "            subjects.append(details['subject'])\n",
    "            years.append(details['year'])\n",
    "            exams.append(details['exam'])\n",
    "            exercises.append(details['exercise'])\n",
    "            statements.append(statement)\n",
    "        # error with extracting details\n",
    "        except:\n",
    "            # pages ending with www.emestrada.org are pages with solution to exercise\n",
    "            if not statement.endswith('www.emestrada.org'):\n",
    "                generate_error_log(file = file, page = index, error_type= 'details', details = statement)\n",
    "                output = f'File {file.stem} parsed but with errors'\n",
    "            continue\n",
    "        \n",
    "    print(output)\n",
    "\n",
    "    # generate the key and content for the exercises dictionary\n",
    "    if details:\n",
    "        key = details['subject'] + ' ' + str(details['year']) + ' ' + topic\n",
    "        exercises_dict[key] = {\n",
    "            'subject' : subjects,\n",
    "            'year' : years,\n",
    "            'topic' : [topic] * len(subjects),\n",
    "            'exam' : exams,\n",
    "            'exercise' : exercises,\n",
    "            'statement' : statements\n",
    "        }\n",
    "        return exercises_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *create_content_dict*\n",
    "Creates a python dictionary with the exam details and its statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_content_dict():\n",
    "    # create the dict from keys\n",
    "    keys = ['subject', 'year', 'topic', 'exam', 'exercise', 'statement']\n",
    "    #content = dict.fromkeys(exercises_dict[first_processed_file].keys())\n",
    "    content = dict.fromkeys(keys)\n",
    "    # fill the dict with empty lists\n",
    "    for key in content.keys():\n",
    "        content[key] = []\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *process_folder*\n",
    "Processes all files within a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(folder:str, csv_file:str = None, return_df: bool = False) -> pd.DataFrame:\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # get the list of pdf files within the folder\n",
    "    folder_Path = Path(folder)\n",
    "    files = sorted( [i for i in Path.iterdir(folder_Path)\n",
    "                        if i.suffix == '.pdf'] )\n",
    "    \n",
    "    # process each pdf file to extract its information\n",
    "    for file in files:\n",
    "        try:\n",
    "            exercises_dict = process_file(file)\n",
    "            # each year contains a dictionary with keys containing lists of values\n",
    "            for year, dict_ in exercises_dict.items():\n",
    "                df = pd.concat([df, pd.DataFrame(dict_)], axis = 0)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # correct wrong subjects\n",
    "    #df.subject = df.subject.apply(lambda x: 'Química'\n",
    "    #                                if x.endswith('mica') else x)\n",
    "    \n",
    "    # export the output of the processed pdf files to a csv file\n",
    "    if csv_file:  # if a file name is provided\n",
    "        output_file = f'./csv/{csv_file}.csv'\n",
    "    else:\n",
    "        output_file = f'./csv/exercises_{folder_Path.stem}.csv'\n",
    "    \n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    if return_df:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('pdf_files/FISICA'),\n",
       " PosixPath('pdf_files/MATES_CCSS'),\n",
       " PosixPath('pdf_files/QUIMICA')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = sorted([i for i in Path('./pdf_files/').iterdir() if i.is_dir()])\n",
    "folders = folders[:-1]\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_files_in_folder(folder:str) -> None:\n",
    "    start, start_formatted = what_time()\n",
    "    send_telegram_message(f'Parsing process started at: {start_formatted}')\n",
    "\n",
    "    subject_df = pd.DataFrame()\n",
    "    folder = Path(folder)\n",
    "    subfolders = sorted(folder.iterdir())\n",
    "    for subfolder in subfolders:\n",
    "        try:\n",
    "            \n",
    "            topic_df = process_folder(subfolder, return_df = True)\n",
    "            print(f'Success processing folder: {subfolder.stem}')\n",
    "            send_telegram_message(f'Success processing folder: {folder.stem}')\n",
    "            subject_df = pd.concat([subject_df, topic_df], axis = 0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            send_telegram_message(f'Error processing folder: {subfolder.stem}' \\\n",
    "                                f'\\n{e}')\n",
    "    \n",
    "    return subject_df\n",
    "\n",
    "    end, end_formatted = what_time()\n",
    "\n",
    "    send_telegram_message(f'Parsing process finished at: {end_formatted}'\\\n",
    "                        f'\\nTime elapsed in minutes: {elapsed_time_minutes(start, end)}')\n",
    "    send_telegram_message('✅ Your files have been processed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success parsing file: 2000\n",
      "Success parsing file: 2001\n",
      "Success parsing file: 2002\n",
      "Success parsing file: 2004\n",
      "Success parsing file: 2006\n",
      "Success parsing file: 2007\n",
      "Success parsing file: 2008\n",
      "Success parsing file: 2009\n",
      "Success parsing file: 2010\n",
      "Success parsing file: 2011\n",
      "Success parsing file: 2013\n",
      "Success parsing file: QUIM T5 2012\n",
      "Success parsing file: QUIM T5 2014\n",
      "Success parsing file: QUIM T5 2015\n",
      "Success parsing file: QUIM T5 2017\n",
      "Success parsing file: QUIM T5 2018\n",
      "Success processing folder: CONTIENE CINÉTICA\n",
      "Success parsing file: 2000 - Ácido Base\n",
      "Success parsing file: 2001 - Ácido Base\n",
      "Success parsing file: 2002 - Ácido Base\n",
      "Success parsing file: 2003 - Ácido Base\n",
      "Success parsing file: 2004 - Ácido Base\n",
      "Success parsing file: 2005 - Ácido Base\n",
      "Success parsing file: 2006 - Ácido Base\n",
      "Success parsing file: 2007 - Ácido Base\n",
      "Success parsing file: 2008 - Ácido Base\n",
      "Success parsing file: 2009 - Ácido Base\n",
      "Success parsing file: 2010 - Ácido Base\n",
      "Success parsing file: 2011 - Ácido Base\n",
      "Success parsing file: 2012 - Ácido Base\n",
      "Success parsing file: 2013 - Ácido Base\n",
      "Success parsing file: 2014 - Ácido Base\n",
      "Success parsing file: 2015 - Ácido Base\n",
      "Success parsing file: 2016 - Ácido Base\n",
      "Success parsing file: 2017 - Ácido Base\n",
      "Success parsing file: 2018 - Ácido Base\n",
      "Success parsing file: 2019 - Ácido Base\n",
      "Success parsing file: 2020 - Ácido Base\n",
      "Success parsing file: 2021 - Ácido Base\n",
      "Success parsing file: 2022 - Ácido Base\n",
      "Success parsing file: 2023 - Ácido Base\n",
      "Success parsing file: 2024 - Ácido Base\n",
      "Success processing folder: Ácido Base\n",
      "Success parsing file: 2000 - Cantidad Química\n",
      "Success parsing file: 2001 - Cantidad Química\n",
      "Success parsing file: 2002 - Cantidad Química\n",
      "Success parsing file: 2003 - Cantidad Química\n",
      "Success parsing file: 2004 - Cantidad Química\n",
      "Success parsing file: 2005 - Cantidad Química\n",
      "Success parsing file: 2006 - Cantidad Química\n",
      "Success parsing file: 2007 - Cantidad Química\n",
      "Success parsing file: 2008 - Cantidad Química\n",
      "Success parsing file: 2009 - Cantidad Química\n",
      "Success parsing file: 2010 - Cantidad Química\n",
      "Success parsing file: 2011 - Cantidad Química\n",
      "Success parsing file: 2012 - Cantidad Química\n",
      "Success parsing file: 2013 - Cantidad Química\n"
     ]
    }
   ],
   "source": [
    "QUIMICA_df = process_all_files_in_folder('./pdf_files/QUIMICA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUIMICA_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATES_CCSS_df.loc[MATES_CCSS_df.exam == 'Tunto', 'exam'] = 'Junio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject: ['MATES_CCSS']\n",
      "year: [2001 2002 2003 2004 2005 2006 2007 2009 2010 2011 2012 2013 2014 2015\n",
      " 2016 2008 2017 2018 2019 2020 2021 2022 2023 2024]\n",
      "topic: ['Sistemas de ecuaciones lineales' 'Contraste De Hipótesis' 'Probabilidad'\n",
      " 'Inferencia Estadística' 'Funciones CCSS' 'Programación Lineal'\n",
      " 'Matrices Y Determinantes']\n",
      "exam: ['Reserva 4' 'Septiembre' 'Junio' 'Reserva 2' 'Reserva 3' 'Reserva 1'\n",
      " 'Ponencia' 'Julio' 'Modelo']\n",
      "exercise: ['Ejercicio 1 Opción B' 'Ejercicio 1 Opción A' 'Ejercicio 1 Opcion A'\n",
      " 'Ejercicio 1' 'Ejercicio 2' 'Ejercicio 3' 'Ejercicio 4' 'Ejercicio 5'\n",
      " 'Ejercicio 6' 'Ejercicio 7' 'Ejercicio 8' 'Ejercicio 9' 'Ejercicio 10'\n",
      " 'Ejercicio 11' 'Ejercicio 12' 'Ejercicio 13' 'Ejercicio 14'\n",
      " 'Ejercicio 15' 'Ejercicio 16' 'Ejercicio 17' 'Ejercicio 18'\n",
      " 'Ejercicio 19' 'Ejercicio 20' 'Ejercicio 21' 'Ejercicio 22'\n",
      " 'Ejercicio 23' 'Ejercicio 24' 'Ejercicio 25' 'Ejercicio 26'\n",
      " 'Ejercicio 27' 'Ejercicio 4 Opción B' 'Ejercicio 4 Opcióon A'\n",
      " 'Ejercicio 4 Opción A' 'Ejercicio 4 Opcion A' 'Ejercicio 4 Opcion B'\n",
      " 'Ejercicio 3 Parte I Opción A' 'Ejercicio 3 Parte I Opción B'\n",
      " 'Ejercicio 3 Parte I Opcióon A' 'Ejercicio 3 Parte I Opcion A'\n",
      " 'Ejercicio 3 Parte 1 Opción B' '4 Ejercicio 3 Parte I Opción A'\n",
      " 'Ejercicio 3 Opción A' 'Ejercicio 3 Opción B' 'Ejercicio 3 Opcion A'\n",
      " 'Ejercicio 3 Opcióon A' 'Ejercicio 3 Opcion B' 'Ejercicio Cs'\n",
      " 'Ejercicio C6' 'Ejercicio Cs5' 'Ejercicio C5'\n",
      " 'Ejercicio 3 Parte It Opción A' 'Ejercicio 3 Parte Ii Opción B'\n",
      " 'Ejercicio 3 Parte Ii Opción A' 'Ejercicio 3 Parte Iil Opción B'\n",
      " 'Ejercicio 3 Parte Il Opcion A' 'Ejercicio 3 Parte Il Opción B'\n",
      " 'Ejercicio 3 Parte Il Opción A' 'Ejercicio 3 Parte Ii Opcióon A'\n",
      " 'Ejercicio 3 Parte Iil Opción A' 'Ejercicio 3 Parte Il Opcióon A'\n",
      " 'Ejercicio 3 Parte Ii Opcion A' 'Ejercicio D7' 'Ejercicio D8'\n",
      " 'Ejercicio 2 Opción A' 'Ejercicio 2 Opción B' 'Ejercicio 2 Opcion A'\n",
      " 'Ejercicio 2 Opcion B' 'Ejercicio 2 Opcióon A' '16' '17' '18' '19' '20'\n",
      " '21' 'Ejercicio B3' 'Ejercicio B4' 'Ejercicio 1 Opcion B'\n",
      " 'Ejercicio 1B Opción B' '9' '10' '11' '12' '13' '14' '15' 'Ejercicio A2'\n",
      " 'Ejercicio A1' ' Ejercicio 1 Opcion A' '1' '2' '3' '4' '5' '6' '7' '$']\n"
     ]
    }
   ],
   "source": [
    "for col in MATES_CCSS_df.columns[:-1]:\n",
    "    print(f'{col}: {MATES_CCSS_df[col].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATES_CCSS_df.to_csv('./csv/MATES_CCSS.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get random files to test parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice, seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_topics(files_per_folder:int = 5) -> list:\n",
    "    sample_files = []\n",
    "    seed(42)\n",
    "    subjects = sorted([i for i in Path('./pdf_files/').iterdir() if i.is_dir()])\n",
    "    # remove samples folder\n",
    "    subjects = subjects[:-1]\n",
    "\n",
    "    sample_files = []\n",
    "    for s in subjects:\n",
    "        for subfolder in s.iterdir():\n",
    "            print(f'Processing {subfolder.stem}')\n",
    "            files = list(subfolder.glob('**/*.pdf'))\n",
    "            sample_files.extend(choice(files, files_per_folder, replace = False))  \n",
    "\n",
    "    print(f'{len(sample_files)} files sampled')\n",
    "\n",
    "    return sample_files \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_subjects(files_per_folder:int = 3) -> list:\n",
    "    sample_files = []\n",
    "    seed(42)\n",
    "\n",
    "    for f in folders:\n",
    "        \n",
    "        files = list(f.glob(\"**/*.pdf\"))\n",
    "        sample_files.extend(choice(files,files_per_folder, replace=False))\n",
    "\n",
    "    print(f'Files selected: {len(sample_files)}')\n",
    "    \n",
    "    return sorted(sample_files)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files = sample_subjects(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files = sample_topics(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the sample files into folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('pdf_files/sample_files', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in sample_files:\n",
    "    shutil.copy(file, f'pdf_files/sample_files/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the sample files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_folder('pdf_files/sample_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns[:-1]:\n",
    "    print(f'{col}: {df[col].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pdf_to_text('pdf_files/folder/2003 - Conf. Electrónica.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = extract_statement(pages[0])\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_exam_details(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/exercises_sample_files.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check processing of files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create path to file to process\n",
    "2. Use `pdf_to_text` to process the pdf file. Store output in `pages`\n",
    "3. Extract statement using `extract_statement`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = Path('pdf_files/MATES_CCSS/Contraste de Hipótesis/2016 - Contraste De Hipótesis.pdf')\n",
    "pages = pdf_to_text(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = extract_statement(pages[0])\n",
    "statement.split('\\n')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = extract_exam_details(statement)\n",
    "details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement.split('\\n')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Addition of exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load csv into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pdf.read_csv('./csv/exercises_Funciones CCSS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = '''Se consideran las matrices:\n",
    "7 -6 —2\n",
    "A=|1 3 1 4\n",
    "-5 0\n",
    "a) Resuelva la ecuación matricial A' — X - A=3-1,.\n",
    "b) ¿Existe algún valor del parámetro a para el que se verifique C' - D = B ?. En caso afirmativo,\n",
    "calcule dicho valor.'''\n",
    "\n",
    "\n",
    "# SOCIALES II. 2017 JUNIO. EJERCICIO 2. OPCIÓN A\n",
    "exam_details = 'SOCIALES II. 2022 RESERVA 4. EJERCICIO A2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 'MATES_CCSS',\n",
       " 'year': 2022,\n",
       " 'exam': 'Reserva 4',\n",
       " 'exercise': 'Ejercicio A2'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_exam_details(exam_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_row(MATES_CCSS_df, statement, exam_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.topic.fillna('Matrices y Determinantes', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'/home/daniel/git_code/emestrada/csv/MATES CCSS/MATES_CCSS.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crete combined csv file for subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [i for i in Path.iterdir(Path('./csv'))\n",
    "             if i.suffix == '.csv' and not i.stem.startswith('all')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(csv_files[0])\n",
    "# for file in csv_files[1:]:\n",
    "#     df = pd.concat([df, pd.read_csv(file)], axis = 0)\n",
    "#     #df.to_csv('./csv/all_exercises.csv', index=False)\n",
    "\n",
    "# df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df.year.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.subject.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.exercise.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.exercise = df.exercise.apply(lambda x: x.replace('Opcióon', 'Opción').replace('Opcion', 'Opción'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_comma(string):\n",
    "    if len(string.split(' ')) == 4 and ',' not in string:\n",
    "        words = string.split(' ')\n",
    "        return f'{words[0]} {words[1]}, {words[2]} {words[3]}'\n",
    "    else:\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.exercise = df.exercise.apply(add_comma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'./csv/all_exercises_{df.subject.unique()[0]}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
