{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selectividad Exams Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required packages for OCR processing:\n",
    "**Ubuntu based:**\n",
    "- tesseract-ocr\n",
    "- tesseract-ocr-spa\n",
    "- poppler-utils\n",
    "\n",
    "**Archlinux based:**\n",
    "- tesseract\n",
    "- tesseract-data-spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdf2image\n",
    "# !pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telegram bot\n",
    "\n",
    "Function to generate messages to a telegram bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your bot's token and chat ID\n",
    "from bot_credentials import *\n",
    "\n",
    "def send_telegram_message(message):\n",
    "    url = f\"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage\"\n",
    "    params = {\"chat_id\": CHAT_ID, \"text\": message}\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *pdf_to_text*\n",
    "\n",
    "Extract the text from pdf files, page by page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(file_path):\n",
    "    images = convert_from_path(file_path)[1:]\n",
    "    # parse each page\n",
    "    pages = []\n",
    "    for i, page in enumerate(images):\n",
    "        pages.append(pytesseract.image_to_string(page, lang='spa'))\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *extract_statement*\n",
    "Extract the statement from a page in text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statement_physics(page):\n",
    "    # split the content into lines\n",
    "    text = page.split('\\n')\n",
    "    # list to store the processed lines\n",
    "    final_text = []\n",
    "    # start from the 4th line\n",
    "    for line in text[4:]:\n",
    "        # stop at RESOLUCION\n",
    "        if line.startswith('FISICA'):\n",
    "            final_text.append(line)\n",
    "            break\n",
    "        # add the line to final_text\n",
    "        else:\n",
    "            if not line == '':\n",
    "                final_text.append(line.strip())\n",
    "    # join the list into a single string\n",
    "    final_text = '\\n'.join(final_text)\n",
    "\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *extract_exam_details*\n",
    "Extract the exam details, like subject, year, exam and exercise number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_exam_details(statement):\n",
    "    # get last line with exam details\n",
    "    lines = [i for i in statement.split('\\n') if i != '']\n",
    "    exam_details = lines[-1].lower()\n",
    "   \n",
    "    # SOCIALES II. 2017 JUNIO. EJERCICIO 2. OPCIÓN A\n",
    "    if exam_details.startswith('sociales'):\n",
    "        exam_details = exam_details.replace('sociales il', 'MATES_CCSS')\n",
    "    # split the words by ' '\n",
    "    exam_details = exam_details.split(' ')\n",
    "    # strip whitespaces\n",
    "    exam_details = [i.replace('.', '')\n",
    "                    .replace(',', '')\n",
    "                    .replace('(', '')\n",
    "                    .replace(')', '').strip()\n",
    "                    for i in exam_details]\n",
    "\n",
    "    # exam dictionary\n",
    "    details = ['subject', 'year', 'exam', 'exercise']\n",
    "    exam_dict = dict.fromkeys(details)\n",
    "    exam_dict = {k:None for k in details}\n",
    "\n",
    "    ## fill dictionary with values\n",
    "    \n",
    "    # subject\n",
    "    if exam_details[0] == 'sociales':\n",
    "        exam_dict['subject'] = 'Mates CCSS'\n",
    "        # drop 2nd element of exam_details list\n",
    "        del exam_details[1]\n",
    "    else:\n",
    "        exam_dict['subject'] = exam_details[0]\n",
    "    \n",
    "    # year\n",
    "    # SOCIALES II. PONENCIA 2009. EJERCICIO 2\n",
    "    if exam_details[1] == 'ponencia':\n",
    "        # switch ponencia and year\n",
    "        exam_details[1], exam_details[2] = exam_details[2], exam_details[1]\n",
    "\n",
    "    exam_dict['year'] = int(exam_details[1])\n",
    "\n",
    "    # parse the exam string\n",
    "    if exam_details[2].startswith('reserva'):\n",
    "        exam_dict['exam'] = ' '.join(exam_details[2:4]).title()\n",
    "        exam_index_start = 4\n",
    "    else:\n",
    "        exam_dict['exam'] = exam_details[2].title()\n",
    "        exam_index_start = 3\n",
    "    \n",
    "    exam_dict['exercise'] = ' '.join(exam_details[exam_index_start:]).title()\n",
    "\n",
    "    \n",
    "    return exam_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = Path('pdf_files/FÍSICA/ONDULATORIO/2021 - Movimiento Ondulatorio.pdf')\n",
    "# pages = pdf_to_text(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st = extract_statement(pages[0])\n",
    "# print(st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_exam_details(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_exam_details_physics(statement):\n",
    "    #FISICA. 2021. RESERVA 1. EJERCICIO C2\n",
    "    # get last line with exam details\n",
    "    lines = [i for i in statement.split('\\n') if i != '']\n",
    "    exam_details = lines[-1].lower()\n",
    "    \n",
    "    # MATES CCSS\n",
    "    # sample line: SOCIALES II. 2017 JUNIO. EJERCICIO 2. OPCIÓN A\n",
    "    if exam_details.startswith('sociales'):\n",
    "        exam_details = exam_details.replace('sociales il', 'MATES_CCSS')\n",
    "    # split the words by ' '\n",
    "    exam_details = exam_details.split(' ')\n",
    "    # strip whitespaces\n",
    "    exam_details = [i.replace('.', '')\n",
    "                    .replace(',', '')\n",
    "                    .replace('(', '')\n",
    "                    .replace(')', '').strip()\n",
    "                    for i in exam_details]\n",
    "\n",
    "    # exam dictionary\n",
    "    details = ['subject', 'year', 'exam', 'exercise']\n",
    "    exam_dict = dict.fromkeys(details)\n",
    "    exam_dict = {k:None for k in details}\n",
    "\n",
    "    ## fill dictionary with values\n",
    "    \n",
    "    # subject\n",
    "    exam_dict['subject'] = exam_details[0]\n",
    "    \n",
    "    # year\n",
    "\n",
    "    # sample line: SOCIALES II. PONENCIA 2009. EJERCICIO 2\n",
    "    # fix for PONENCIA exams\n",
    "    if exam_details[1] == 'ponencia':\n",
    "        # switch ponencia and year\n",
    "        exam_details[1], exam_details[2] = exam_details[2], exam_details[1]\n",
    "\n",
    "    exam_dict['year'] = int(exam_details[1])\n",
    "\n",
    "    # parse the exam string\n",
    "    if exam_details[2].startswith('reserva'):\n",
    "        exam_dict['exam'] = ' '.join(exam_details[2:4]).title()\n",
    "        exam_index_start = 4\n",
    "    else:\n",
    "        exam_dict['exam'] = exam_details[2].title()\n",
    "        exam_index_start = 3\n",
    "    \n",
    "    exam_dict['exercise'] = ' '.join(exam_details[exam_index_start:]).title()\n",
    "\n",
    "    \n",
    "    return exam_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *time*\n",
    "Function to measure the time taken to process the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def what_time():\n",
    "    now = time.localtime()\n",
    "    time_formated = time.strftime(\"%H:%M:%S on %Y-%m-%d \", now)\n",
    "    return now, time_formated\n",
    "\n",
    "def elapsed_time_minutes(start, end):\n",
    "    start_seconds = time.mktime(start)\n",
    "    end_seconds = time.mktime(end)\n",
    "    return (end_seconds - start_seconds) / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *generate_error_log*\n",
    "generate a error log and add it to errors_timestamp.log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_error_log(file, page, error_type, statement = None, details = None):\n",
    "    log = f'Error processing {error_type}, in {file.name} at page {page+2}\\n'\n",
    "    \n",
    "    # add timestamp to log file\n",
    "    current_datetime = datetime.now()\n",
    "    formatted_dt = current_datetime.strftime(\"%d-%m-%Y\")\n",
    "    with open(f'./error_logs/errors_{formatted_dt}.log', 'a') as f:\n",
    "        f.write(log)\n",
    "        f.write('*' * 10 + '\\n')\n",
    "        if statement:\n",
    "            f.write(f'{statement}\\n')\n",
    "        elif details:\n",
    "            f.write(f'{details}\\n')\n",
    "        f.write('*' * 10 + '\\n')\n",
    "    \n",
    "    # print log to console without the new line\n",
    "    print(log[:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *add_row*\n",
    "Manually add a row to the processed exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_row(df, statement, exam_details):\n",
    "    \n",
    "    row = extract_exam_details(exam_details)\n",
    "    row['statement'] = statement\n",
    "\n",
    "    return pd.concat([df, pd.DataFrame(row, columns = df.columns, index = [0])], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *process_file*\n",
    "Process a pdf files, extracting its information: statement and exam information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file):\n",
    "    exercises_dict = {}\n",
    "    topic = file.stem.split(' - ')[-1]   \n",
    "    \n",
    "    # lists to store the values\n",
    "    subjects = []\n",
    "    years = []\n",
    "    exams = []\n",
    "    exercises = []\n",
    "    statements = []\n",
    "    pages = pdf_to_text(file)\n",
    "\n",
    "    # parse each page in the pdf file\n",
    "\n",
    "    for index, page in enumerate(pages):\n",
    "\n",
    "        # extract statement from page\n",
    "        try:\n",
    "            statement = extract_statement_physics(page)\n",
    "        # error with extracting statement\n",
    "        except:\n",
    "            if statement.endswith('www.emestrada.org'):\n",
    "                continue\n",
    "            generate_error_log(file = file, page = index, error_type= 'statement', statement=page)\n",
    "\n",
    "        # extract exam details\n",
    "        try:\n",
    "            details = extract_exam_details(statement)\n",
    "            subjects.append(details['subject'])\n",
    "            years.append(details['year'])\n",
    "            exams.append(details['exam'])\n",
    "            exercises.append(details['exercise'])\n",
    "            statements.append(statement)\n",
    "        # error with extracting details\n",
    "        except:\n",
    "            # pages ending with www.emestrada.org are pages with solution to exercise\n",
    "            if not statement.endswith('www.emestrada.org'):\n",
    "                generate_error_log(file = file, page = index, error_type= 'details', details = statement)\n",
    "            continue\n",
    "        \n",
    "    print(f'Success parsing file: {file.stem}')\n",
    "\n",
    "    # generate the key and content for the exercises dictionary\n",
    "    key = details['subject'] + ' ' + str(details['year']) + ' ' + topic\n",
    "    exercises_dict[key] = {\n",
    "        'subject' : subjects,\n",
    "        'year' : years,\n",
    "        'topic' : [topic] * len(subjects),\n",
    "        'exam' : exams,\n",
    "        'exercise' : exercises,\n",
    "        'statement' : statements\n",
    "    }\n",
    "\n",
    "    return exercises_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *create_content_dict*\n",
    "Creates a python dictionary with the exam details and its statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_content_dict():\n",
    "    # create the dict from keys\n",
    "    keys = ['subject', 'year', 'topic', 'exam', 'exercise', 'statement']\n",
    "    #content = dict.fromkeys(exercises_dict[first_processed_file].keys())\n",
    "    content = dict.fromkeys(keys)\n",
    "    # fill the dict with empty lists\n",
    "    for key in content.keys():\n",
    "        content[key] = []\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(folder):\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # get the list of pdf files within the folder\n",
    "    files = sorted( [i for i in Path.iterdir(folder)\n",
    "                        if i.suffix == '.pdf'] )\n",
    "    \n",
    "    # process each pdf file to extract its information\n",
    "    for file in files:\n",
    "        try:\n",
    "            exercises_dict = process_file(file)\n",
    "            # each year contains a dictionary with keys containing lists of values\n",
    "            for year, dict_ in exercises_dict.items():\n",
    "                df = pd.concat([df, pd.DataFrame(dict_)], axis = 0)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # correct wrong subjects\n",
    "    #df.subject = df.subject.apply(lambda x: 'Química'\n",
    "    #                                if x.endswith('mica') else x)\n",
    "    \n",
    "    # export the output of the processed pdf files to a csv file\n",
    "    df.to_csv(f'./csv/exercises_{folder.stem}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = sorted([i for i in Path('./pdf_files/FÍSICA/').iterdir() if i.is_dir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('pdf_files/FÍSICA/Campo eléctrico y magnético'),\n",
       " PosixPath('pdf_files/FÍSICA/Campo gravitatorio'),\n",
       " PosixPath('pdf_files/FÍSICA/Física cuántica y nuclear'),\n",
       " PosixPath('pdf_files/FÍSICA/Movimiento Ondulatorio'),\n",
       " PosixPath('pdf_files/FÍSICA/Ondas'),\n",
       " PosixPath('pdf_files/FÍSICA/Óptica geométrica')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success parsing file: 2016 - Campo eléctrico y magnético\n",
      "Success parsing file: 2017 - Campo eléctrico y magnético\n",
      "Success parsing file: 2018 - Campo eléctrico y magnético\n",
      "Success parsing file: 2019 - Campo eléctrico y magnético\n",
      "Success parsing file: 2020 - Campo eléctrico y magnético\n",
      "Success parsing file: 2021 - Campo eléctrico y magnético\n",
      "Success parsing file: 2022 - Campo eléctrico y magnético\n",
      "Success parsing file: 2023 - Campo eléctrico y magnético\n",
      "Success parsing file: 2024 - Campo eléctrico y magnético\n",
      "Success processing folder: Campo eléctrico y magnético\n",
      "Success parsing file: 2016 - Campo gravitatorio\n",
      "Success parsing file: 2017 - Campo gravitatorio\n",
      "Success parsing file: 2018 - Campo gravitatorio\n",
      "Success parsing file: 2019 - Campo gravitatorio\n",
      "Success parsing file: 2020 - Campo gravitatorio\n",
      "Success parsing file: 2021 - Campo gravitatorio\n",
      "Success parsing file: 2022 - Campo gravitatorio\n",
      "Success parsing file: 2023 - Campo gravitatorio\n",
      "Success parsing file: 2024 - Campo gravitatorio\n",
      "Success processing folder: Campo gravitatorio\n",
      "Success parsing file: 2016 - Física cuántica y nuclear\n",
      "Success parsing file: 2017 - Física cuántica y nuclear\n",
      "Success parsing file: 2018 - Física cuántica y nuclear\n",
      "Success parsing file: 2019 - Física cuántica y nuclear\n",
      "Success parsing file: 2020 - Física cuántica y nuclear\n",
      "Success parsing file: 2021 - Física cuántica y nuclear\n",
      "Success parsing file: 2022 - Física cuántica y nuclear\n",
      "Success parsing file: 2023 - Física cuántica y nuclear\n",
      "Success parsing file: 2024 - Física cuántica y nuclear\n",
      "Success processing folder: Física cuántica y nuclear\n",
      "Success parsing file: 2021 - Movimiento Ondulatorio\n",
      "Success parsing file: 2022 - Movimiento Ondulatorio\n",
      "Success parsing file: 2023 - Movimiento Ondulatorio\n",
      "Success parsing file: 2024 - Movimiento Ondulatorio\n",
      "Success processing folder: Movimiento Ondulatorio\n",
      "Success parsing file: 2016 - Ondas\n",
      "Success parsing file: 2017 - Ondas\n",
      "Success parsing file: 2018 - Ondas\n",
      "Success parsing file: 2019 - Ondas\n",
      "Success parsing file: 2020 - Ondas\n",
      "Success parsing file: 2021 - Ondas\n",
      "Success parsing file: 2022 - Ondas\n",
      "Success parsing file: 2023 - Ondas\n",
      "Success parsing file: 2024 - Ondas\n",
      "Success processing folder: Ondas\n",
      "Success parsing file: 2016 - Óptica geométrica\n",
      "Success parsing file: 2017 - Óptica geométrica\n",
      "Success parsing file: 2018 - Óptica geométrica\n",
      "Success parsing file: 2019 - Óptica geométrica\n",
      "Success parsing file: 2020 - Óptica geométrica\n",
      "Success parsing file: 2021 - Óptica geométrica\n",
      "Success parsing file: 2022 - Óptica geométrica\n",
      "Success parsing file: 2023 - Óptica geométrica\n",
      "Success parsing file: 2024 - Óptica geométrica\n",
      "Success processing folder: Óptica geométrica\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ok': True,\n",
       " 'result': {'message_id': 213,\n",
       "  'from': {'id': 7874470313,\n",
       "   'is_bot': True,\n",
       "   'first_name': 'notifyBot',\n",
       "   'username': 'alert294Bot'},\n",
       "  'chat': {'id': 6188540451, 'first_name': 'Dani', 'type': 'private'},\n",
       "  'date': 1742033765,\n",
       "  'text': '✅ Your files have been processed!'}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, start_formatted = what_time()\n",
    "send_telegram_message(f'Parsing process started at: {start_formatted}')\n",
    "\n",
    "for folder in folders:\n",
    "    try:\n",
    "        process_folder(folder)\n",
    "        print(f'Success processing folder: {folder.stem}')\n",
    "        send_telegram_message(f'Success processing folder: {folder.stem}')\n",
    "    except Exception as e:\n",
    "        send_telegram_message(f'Error processing folder: {folder.stem}' \\\n",
    "                              f'\\n{e}')\n",
    "end, end_formatted = what_time()\n",
    "\n",
    "send_telegram_message(f'Parsing process finished at: {end_formatted}'\\\n",
    "                      f'\\nTime elapsed in minutes: {elapsed_time_minutes(start, end)}')\n",
    "send_telegram_message('✅ Your files have been processed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Addition of exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load csv into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/daniel/git_code/emestrada'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmptyDataError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./csv/FÍSICA/exercises_Campo gravitatorio.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.13/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:581\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mEmptyDataError\u001b[39m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./csv/FÍSICA/exercises_Campo gravitatorio.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = '''En un aula de dibujo hay 40 sillas, 30 con respaldo y 10 sin él. Entre las sillas sin respaldo hay 3\n",
    "nuevas y entre las sillas con respaldo hay 7 nuevas.\n",
    "a) Tomada una silla al azar, ¿cuál es la probabilidad de que sea nueva?.\n",
    "b) Si se coge una silla que no es nueva, ¿cuál es la probabilidad de que no tenga respaldo?'''\n",
    "\n",
    "# SOCIALES II. 2017 JUNIO. EJERCICIO 2. OPCIÓN A\n",
    "exam_details = 'SOCIALES II. 2006 JUNIO. EJERCICIO 3. OPCIÓN A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 'Mates CCSS',\n",
       " 'year': 2006,\n",
       " 'exam': 'Junio',\n",
       " 'exercise': 'Ejercicio 3 Opción A'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_exam_details(exam_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = add_row(\u001b[43mdf\u001b[49m, statement, exam_details)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df = add_row(df, statement, exam_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'./csv/', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crete combined csv file for subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [i for i in Path.iterdir(Path('./csv'))\n",
    "             if i.suffix == '.csv' and not i.stem.startswith('all')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(csv_files[0])\n",
    "# for file in csv_files[1:]:\n",
    "#     df = pd.concat([df, pd.read_csv(file)], axis = 0)\n",
    "#     #df.to_csv('./csv/all_exercises.csv', index=False)\n",
    "\n",
    "# df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df.year.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.subject.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.exercise.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.exercise = df.exercise.apply(lambda x: x.replace('Opcióon', 'Opción').replace('Opcion', 'Opción'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_comma(string):\n",
    "    if len(string.split(' ')) == 4 and ',' not in string:\n",
    "        words = string.split(' ')\n",
    "        return f'{words[0]} {words[1]}, {words[2]} {words[3]}'\n",
    "    else:\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.exercise = df.exercise.apply(add_comma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'./csv/all_exercises_{df.subject.unique()[0]}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
