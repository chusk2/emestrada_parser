{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selectividad Exams Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required packages for OCR processing:\n",
    "**Ubuntu based:**\n",
    "- tesseract-ocr\n",
    "- tesseract-ocr-spa\n",
    "- poppler-utils\n",
    "\n",
    "**Archlinux based:**\n",
    "- tesseract\n",
    "- tesseract-data-spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdf2image\n",
    "# !pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telegram bot\n",
    "\n",
    "Function to generate messages to a telegram bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your bot's token and chat ID\n",
    "from bot_credentials import *\n",
    "\n",
    "def send_telegram_message(message):\n",
    "    url = f\"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage\"\n",
    "    params = {\"chat_id\": CHAT_ID, \"text\": message}\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *pdf_to_text*\n",
    "\n",
    "Extract the text from pdf files, page by page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(file_path):\n",
    "    \"\"\"\n",
    "    Processes a single PDF file to extract exam information and statements.\n",
    "\n",
    "    Args:\n",
    "        file (Path): A Path object representing the PDF file to be processed.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted information with the following structure:\n",
    "            {\n",
    "                \"key\": {\n",
    "                    \"subject\": list of subjects,\n",
    "                    \"year\": list of years,\n",
    "                    \"topic\": list of topics,\n",
    "                    \"exam\": list of exams,\n",
    "                    \"exercise\": list of exercises,\n",
    "                    \"statement\": list of statements\n",
    "                }\n",
    "            }\n",
    "\n",
    "    Workflow:\n",
    "        1. Extracts the topic from the file name.\n",
    "        2. Converts the PDF file into text, page by page.\n",
    "        3. For each page:\n",
    "            - Extracts the statement using the `extract_statement` function.\n",
    "            - Extracts exam details using the `extract_exam_details` function.\n",
    "            - Handles errors by logging them using `generate_error_log`.\n",
    "        4. Aggregates the extracted information into lists for subjects, years, topics, exams, exercises, and statements.\n",
    "        5. Returns a dictionary with the extracted data.\n",
    "\n",
    "    Notes:\n",
    "        - Logs errors to a file if there are issues with extracting statements or exam details.\n",
    "        - Skips pages with errors and continues processing the remaining pages.\n",
    "    \"\"\"\n",
    "    images = convert_from_path(file_path)[1:]\n",
    "    # parse each page\n",
    "    pages = []\n",
    "    for i, page in enumerate(images):\n",
    "        pages.append(pytesseract.image_to_string(page, lang='spa'))\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *extract_statement*\n",
    "Extract the statement from a page in text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statement(page):\n",
    "    # split the content into lines\n",
    "    text = page.split('\\n')\n",
    "\n",
    "    # list to store the processed lines\n",
    "    final_text = []\n",
    "    # start from the 4th line\n",
    "    for line in text[4:]:\n",
    "        # stop at RESOLUCION\n",
    "        if line.startswith('RESOLUCIÓN') or line.startswith('RES OLUCION'):\n",
    "            break\n",
    "        # add the line to final_text\n",
    "        else:\n",
    "            if not line == '':\n",
    "                final_text.append(line.strip())\n",
    "    # join the list into a single string\n",
    "    final_text = '\\n'.join(final_text)\n",
    "\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *extract_exam_details*\n",
    "Extract the exam details, like subject, year, exam and exercise number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_exam_details(statement):\n",
    "    # get last line with exam details\n",
    "    lines = [i for i in statement.split('\\n') if i != '']\n",
    "    exam_details = lines[-1].lower()\n",
    "\n",
    "    # strip whitespaces\n",
    "    exam_details = (exam_details.replace('.', '')            \n",
    "                    .replace(',', '')\n",
    "                    .replace('(', '')\n",
    "                    .replace(')', '').strip()\n",
    "    )\n",
    "    \n",
    "    # split the words by ' '\n",
    "    exam_details = exam_details.split(' ')\n",
    "\n",
    "    # MATES CCSS\n",
    "    # SOCIALES II. 2017 JUNIO. EJERCICIO 2. OPCIÓN A\n",
    "    # \n",
    "    if exam_details[0].startswith('sociales'):\n",
    "        exam_details[0] = 'MATES_CCSS'\n",
    "        # remove the second element in the list\n",
    "        del exam_details[1]\n",
    "    \n",
    "       \n",
    "    # exam dictionary\n",
    "    details = ['subject', 'year', 'exam', 'exercise']\n",
    "    exam_dict = dict.fromkeys(details)\n",
    "    exam_dict = {k:None for k in details}\n",
    "\n",
    "    ## fill dictionary with values\n",
    "    \n",
    "    # subject\n",
    "    exam_dict['subject'] = exam_details[0]\n",
    "    \n",
    "    # year\n",
    "    # SOCIALES II. PONENCIA 2009. EJERCICIO 2\n",
    "    if exam_details[1] == 'ponencia':\n",
    "        # switch ponencia and year\n",
    "        exam_details[1], exam_details[2] = exam_details[2], exam_details[1]\n",
    "\n",
    "    exam_dict['year'] = int(exam_details[1])\n",
    "\n",
    "    # parse the exam string\n",
    "    if exam_details[2].startswith('reserva'):\n",
    "        exam_dict['exam'] = ' '.join(exam_details[2:4]).title()\n",
    "        exam_index_start = 4\n",
    "    else:\n",
    "        exam_dict['exam'] = exam_details[2].title()\n",
    "        exam_index_start = 3\n",
    "    \n",
    "    exam_dict['exercise'] = ' '.join(exam_details[exam_index_start:]).title()\n",
    "\n",
    "    \n",
    "    return exam_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *time*\n",
    "Function to measure the time taken to process the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def what_time():\n",
    "    now = time.localtime()\n",
    "    time_formated = time.strftime(\"%H:%M:%S on %Y-%m-%d \", now)\n",
    "    return now, time_formated\n",
    "\n",
    "def elapsed_time_minutes(start, end):\n",
    "    start_seconds = time.mktime(start)\n",
    "    end_seconds = time.mktime(end)\n",
    "    return (end_seconds - start_seconds) / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *generate_error_log*\n",
    "generate a error log and add it to errors_timestamp.log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_error_log(file, page, error_type, statement = None, details = None):\n",
    "    log = f'Error processing {error_type}, in {file.name} at page {page+2}\\n'\n",
    "    \n",
    "    # add timestamp to log file\n",
    "    current_datetime = datetime.now()\n",
    "    formatted_dt = current_datetime.strftime(\"%d-%m-%Y\")\n",
    "    with open(f'./error_logs/errors_{formatted_dt}.log', 'a') as f:\n",
    "        f.write(log)\n",
    "        f.write('*' * 10 + '\\n')\n",
    "        if statement:\n",
    "            f.write(f'{statement}\\n')\n",
    "        elif details:\n",
    "            f.write(f'{details}\\n')\n",
    "        f.write('*' * 10 + '\\n')\n",
    "    \n",
    "    # print log to console without the new line\n",
    "    print(log[:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *add_row*\n",
    "Manually add a row to the processed exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_row(df, statement, exam_details):\n",
    "    \n",
    "    row = extract_exam_details(exam_details)\n",
    "    row['statement'] = statement\n",
    "\n",
    "    return pd.concat([df, pd.DataFrame(row, columns = df.columns, index = [0])], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *process_file*\n",
    "Process a pdf files, extracting its information: statement and exam information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file):\n",
    "    exercises_dict = {}\n",
    "    topic = file.stem.split(' - ')[-1]   \n",
    "    \n",
    "    # lists to store the values\n",
    "    subjects = []\n",
    "    years = []\n",
    "    exams = []\n",
    "    exercises = []\n",
    "    statements = []\n",
    "    pages = pdf_to_text(file)\n",
    "\n",
    "    # parse each page in the pdf file\n",
    "\n",
    "    for index, page in enumerate(pages):\n",
    "\n",
    "        # extract statement from page\n",
    "        try:\n",
    "            statement = extract_statement(page)\n",
    "        # error with extracting statement\n",
    "        except:\n",
    "            generate_error_log(file = file, page = index, error_type= 'statement', statement=page)\n",
    "            continue\n",
    "\n",
    "        # extract exam details\n",
    "        try:\n",
    "            details = extract_exam_details(statement)\n",
    "            subjects.append(details['subject'])\n",
    "            years.append(details['year'])\n",
    "            exams.append(details['exam'])\n",
    "            exercises.append(details['exercise'])\n",
    "            statements.append(statement)\n",
    "        # error with extracting details\n",
    "        except:\n",
    "            # pages ending with www.emestrada.org are pages with solution to exercise\n",
    "            if not statement.endswith('www.emestrada.org'):\n",
    "                generate_error_log(file = file, page = index, error_type= 'details', details = statement)\n",
    "            continue\n",
    "        \n",
    "    print(f'Success parsing file: {file.stem}')\n",
    "\n",
    "    # generate the key and content for the exercises dictionary\n",
    "    key = details['subject'] + ' ' + str(details['year']) + ' ' + topic\n",
    "    exercises_dict[key] = {\n",
    "        'subject' : subjects,\n",
    "        'year' : years,\n",
    "        'topic' : [topic] * len(subjects),\n",
    "        'exam' : exams,\n",
    "        'exercise' : exercises,\n",
    "        'statement' : statements\n",
    "    }\n",
    "\n",
    "    return exercises_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *create_content_dict*\n",
    "Creates a python dictionary with the exam details and its statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_content_dict():\n",
    "    # create the dict from keys\n",
    "    keys = ['subject', 'year', 'topic', 'exam', 'exercise', 'statement']\n",
    "    #content = dict.fromkeys(exercises_dict[first_processed_file].keys())\n",
    "    content = dict.fromkeys(keys)\n",
    "    # fill the dict with empty lists\n",
    "    for key in content.keys():\n",
    "        content[key] = []\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(folder):\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # get the list of pdf files within the folder\n",
    "    files = sorted( [i for i in Path.iterdir(folder)\n",
    "                        if i.suffix == '.pdf'] )\n",
    "    \n",
    "    # process each pdf file to extract its information\n",
    "    for file in files:\n",
    "        try:\n",
    "            exercises_dict = process_file(file)\n",
    "            # each year contains a dictionary with keys containing lists of values\n",
    "            for year, dict_ in exercises_dict.items():\n",
    "                df = pd.concat([df, pd.DataFrame(dict_)], axis = 0)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # correct wrong subjects\n",
    "    #df.subject = df.subject.apply(lambda x: 'Química'\n",
    "    #                                if x.endswith('mica') else x)\n",
    "    \n",
    "    # export the output of the processed pdf files to a csv file\n",
    "    df.to_csv(f'./csv/exercises_{folder.stem}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = sorted([i for i in Path('./pdf_files/').iterdir() if i.is_dir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('pdf_files/FISICA'),\n",
       " PosixPath('pdf_files/MATES_CCSS'),\n",
       " PosixPath('pdf_files/QUIMICA')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get random files to test parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf_files/QUIMICA/Ácido Base/2004 - Ácido Base.pdf\n",
      "pdf_files/FISICA/Óptica geométrica/2021 - Óptica geométrica.pdf\n",
      "pdf_files/QUIMICA/Enlace Químico/2009 - Enlace Químico.pdf\n",
      "pdf_files/QUIMICA/Reactividad Orgánica/2002 - Reactividad Orgánica.pdf\n",
      "pdf_files/QUIMICA/Ácido Base/2006 - Ácido Base.pdf\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "\n",
    "for f in folders:\n",
    "    files.extend( list(f.glob(\"**/*.pdf\")) )\n",
    "\n",
    "print(*choice(files,5), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([PosixPath('pdf_files/MATES_CCSS/Matrices y Determinantes/2015 - Matrices Y Determinantes.pdf'),\n",
       "       PosixPath('pdf_files/MATES_CCSS/Funciones CCSS/2002 - Funciones CCSS.pdf'),\n",
       "       PosixPath('pdf_files/MATES_CCSS/Probabilidad/2023 - Probabilidad.pdf'),\n",
       "       PosixPath('pdf_files/QUIMICA/Cantidad Química/2002 - Cantidad Química.pdf'),\n",
       "       PosixPath('pdf_files/FISICA/Movimiento Ondulatorio/2016 - Ondas.pdf')],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice(files,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files = []\n",
    "for f in files:\n",
    "    sample_files.extend(choice(files, 2))\n",
    "\n",
    "sample_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing details, in 2016 - Campo eléctrico y magnético.pdf at page 1\n",
      "Success parsing file: 2016 - Campo eléctrico y magnético\n",
      "Success parsing file: 2017 - Campo eléctrico y magnético\n",
      "Success parsing file: 2018 - Campo eléctrico y magnético\n",
      "Success parsing file: 2019 - Campo eléctrico y magnético\n",
      "Success parsing file: 2020 - Campo eléctrico y magnético\n",
      "Success parsing file: 2021 - Campo eléctrico y magnético\n",
      "Success parsing file: 2022 - Campo eléctrico y magnético\n",
      "Success parsing file: 2023 - Campo eléctrico y magnético\n",
      "Success parsing file: 2024 - Campo eléctrico y magnético\n",
      "Success processing folder: Campo eléctrico y magnético\n",
      "Success parsing file: 2016 - Campo gravitatorio\n"
     ]
    }
   ],
   "source": [
    "start, start_formatted = what_time()\n",
    "send_telegram_message(f'Parsing process started at: {start_formatted}')\n",
    "\n",
    "for folder in folders:\n",
    "    try:\n",
    "        process_folder(folder)\n",
    "        print(f'Success processing folder: {folder.stem}')\n",
    "        send_telegram_message(f'Success processing folder: {folder.stem}')\n",
    "    except Exception as e:\n",
    "        send_telegram_message(f'Error processing folder: {folder.stem}' \\\n",
    "                              f'\\n{e}')\n",
    "end, end_formatted = what_time()\n",
    "\n",
    "send_telegram_message(f'Parsing process finished at: {end_formatted}'\\\n",
    "                      f'\\nTime elapsed in minutes: {elapsed_time_minutes(start, end)}')\n",
    "send_telegram_message('✅ Your files have been processed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check processing of files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create path to file to process\n",
    "2. Use `pdf_to_text` to process the pdf file. Store output in `pages`\n",
    "3. Extract statement using `extract_statement`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = Path('pdf_files/MATES_CCSS/Contraste de Hipótesis/2016 - Contraste De Hipótesis.pdf')\n",
    "pages = pdf_to_text(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SOCIALES II. 2016 RESERVA 1. EJERCICIO 4 OPCIÓN B'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement = extract_statement(pages[0])\n",
    "statement.split('\\n')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 'MATES_CCSS',\n",
       " 'year': 2016,\n",
       " 'exam': 'Reserva 1',\n",
       " 'exercise': 'Ejercicio 4 Opción B'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details = extract_exam_details(statement)\n",
    "details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SOCIALES II. PONENCIA 2009. EJERCICIO 1'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement.split('\\n')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Addition of exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load csv into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pdf.read_csv('./csv/exercises_Funciones CCSS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = '''a) Calcule los valores de a y b para que la función f(x)= 2-x , sea\n",
    "ax”?-3x+1 si x>1\n",
    "derivable en el punto de abscisa x =1\n",
    "b) Para a=1 y b=2, estudie su monotonía y determine las ecuaciones de sus asíntotas, si\n",
    "existen.'''\n",
    "\n",
    "# SOCIALES II. 2017 JUNIO. EJERCICIO 2. OPCIÓN A\n",
    "exam_details = 'SOCIALES II. 2016 JUNIO. EJERCICIO 2. OPCIÓN A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 'Mates CCSS',\n",
       " 'year': 2006,\n",
       " 'exam': 'Junio',\n",
       " 'exercise': 'Ejercicio 3 Opción A'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_exam_details(exam_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = add_row(\u001b[43mdf\u001b[49m, statement, exam_details)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df = add_row(df, statement, exam_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'./csv/', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crete combined csv file for subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [i for i in Path.iterdir(Path('./csv'))\n",
    "             if i.suffix == '.csv' and not i.stem.startswith('all')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(csv_files[0])\n",
    "# for file in csv_files[1:]:\n",
    "#     df = pd.concat([df, pd.read_csv(file)], axis = 0)\n",
    "#     #df.to_csv('./csv/all_exercises.csv', index=False)\n",
    "\n",
    "# df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df.year.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.subject.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.exercise.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.exercise = df.exercise.apply(lambda x: x.replace('Opcióon', 'Opción').replace('Opcion', 'Opción'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_comma(string):\n",
    "    if len(string.split(' ')) == 4 and ',' not in string:\n",
    "        words = string.split(' ')\n",
    "        return f'{words[0]} {words[1]}, {words[2]} {words[3]}'\n",
    "    else:\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.exercise = df.exercise.apply(add_comma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'./csv/all_exercises_{df.subject.unique()[0]}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
